"""
AI Email Classifier - Phase 2

Uses LangChain + OpenAI/Anthropic to classify emails.
Prompts adapted from proven n8n workflow.
"""
from typing import Optional, Dict, Any, List
from pydantic import BaseModel, Field, ValidationError
from datetime import datetime
import logging
import time
import json
import os

from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.exceptions import OutputParserException
from openai import OpenAI

from backend.core.email.models import ProcessedEmail
from backend.core.ai.config_constants import CHARS_PER_TOKEN_CONSERVATIVE
from backend.core.prompt_loader import get_prompt, get_prompt_variables

logger = logging.getLogger(__name__)

# Token limits for different models
# GPT-4o, GPT-4o-mini, Claude models have 128k context window
# But we want to be conservative and leave room for response + schema
MAX_PROMPT_TOKENS = 100000  # Conservative limit (100k tokens)
MAX_EMAIL_CONTENT_CHARS = 50000  # ~12.5k tokens - enough for classification, not full document analysis
# Lower limits for general classification (vs application-specific processing)
# Classification only needs enough context to identify email type, not evaluate content
# Application processing (pass 2) uses heavier limits: 10 attachments Ã— 20k chars
# General classifier: 3 attachments Ã— 500 chars = 1.5k total (first ~1/2 page per doc)
MAX_ATTACHMENT_SUMMARY_CHARS = 500  # ~125 tokens per attachment - just enough to identify document type
MAX_ATTACHMENTS_FOR_CLASSIFICATION = 3  # Number of attachments to include for general classification


def _truncate_email_content(content: str, max_chars: int = MAX_EMAIL_CONTENT_CHARS) -> tuple[str, bool]:
    """
    Truncate email content intelligently to stay within token limits.
    
    Strategy:
    1. If content is short enough, return as-is
    2. If too long, take first portion and last portion (to capture both context and conclusion)
    3. Add clear truncation marker
    
    Args:
        content: Email content to truncate
        max_chars: Maximum characters to allow
        
    Returns:
        Tuple of (truncated_content, was_truncated)
    """
    if len(content) <= max_chars:
        return content, False
    
    # Take 70% from beginning, 30% from end (with marker in middle)
    first_chunk_size = int(max_chars * 0.7)
    last_chunk_size = int(max_chars * 0.3)
    
    first_chunk = content[:first_chunk_size]
    last_chunk = content[-last_chunk_size:]
    
    truncation_marker = f"\n\n[... {len(content) - max_chars:,} characters truncated for brevity ...]\n\n"
    
    truncated = first_chunk + truncation_marker + last_chunk
    
    return truncated, True


# Nested models for OpenAI structured outputs compatibility
class TechnicalExperience(BaseModel):
    """Technical experience with score and evidence."""
    score: int = Field(ge=0, le=10, description="Experience score 0-10")
    evidence: str = Field(description="Specific evidence for this score")


class ProfileTag(BaseModel):
    """Profile tag with confidence and reason."""
    tag: str = Field(description="Tag name")
    confidence: float = Field(ge=0.0, le=1.0, description="Confidence 0.0-1.0")
    reason: str = Field(description="Why this tag applies")


class InformationUsed(BaseModel):
    """Information sources used for evaluation."""
    email_text: bool = Field(description="Email body was used")
    cv: bool = Field(description="CV/resume was used")
    research_plan: bool = Field(description="Research proposal was used")
    letters_of_recommendation: bool = Field(description="Reference letters were used")
    transcripts: bool = Field(description="Academic transcripts were used")
    other: List[str] = Field(default_factory=list, description="Other documents used")


class AIClassificationResult(BaseModel):
    """
    Structured output from AI classification.
    Maps to n8n workflow outputs with flattened categories.
    """
    # Core Classification (Flattened categories from n8n workflow)
    category: str = Field(description="""One of: work-urgent, work-colleague, work-student, work-admin, work-scheduling, work-no-action-needed, work-other,
application-phd, application-postdoc, application-intern, application-bsc-msc-thesis, application-visiting, application-other,
invitation-speaking, invitation-committee, invitation-grant, invitation-editorial, invitation-advisory, invitation-event, invitation-collaboration,
review-peer-journal, review-peer-conference, review-grant, review-phd-committee, review-hiring, review-promotion, review-other,
publication-submission-confirm, publication-decision-accept, publication-decision-reject, publication-revision-request, publication-proofs, publication-published, publication-other,
grant-submission-confirm, grant-decision-awarded, grant-decision-rejected, grant-budget, grant-reporting, grant-modification, grant-other,
travel-booking-confirm, travel-receipt, travel-itinerary, travel-reminder, travel-transport, travel-other,
newsletter-scientific, newsletter-general, notification-technical, notification-calendar, notification-social, notification-other,
receipt-online, receipt-travel, receipt-subscription, receipt-reimbursement,
personal-family, personal-friends, personal-transaction, personal-health, personal-hobby, personal-travel, personal-shopping, personal-other,
marketing, spam, social-media""")
    confidence: float = Field(ge=0.0, le=1.0, description="0.0-1.0 confidence score")
    reasoning: str = Field(description="Why classified this way")
    
    # Urgency (from n8n prompts)
    urgency: str = Field(description="urgent|normal|low")
    urgency_score: int = Field(ge=1, le=10, description="1-10 urgency rating")
    urgency_reason: str = Field(description="Why this urgency level")
    
    # Summary and Action Items (from n8n)
    summary: str = Field(description="1-2 sentence summary")
    action_items: Optional[List[str]] = Field(default=None, description="Extracted action items")
    
    # Reply Detection (from n8n)
    needs_reply: bool = Field(description="Does this email need a response?")
    reply_deadline: Optional[str] = Field(None, description="YYYY-MM-DD or null")
    reply_suggestion: Optional[str] = Field(None, description="Brief suggestion for reply")
    
    # Sentiment
    sentiment: str = Field(description="positive|neutral|negative")
    
    # Relevance Scores (for invitations/reviews/applications - from n8n)
    relevance_score: Optional[int] = Field(None, ge=1, le=10, description="1-10 relevance to biomedical informatics research")
    relevance_reason: Optional[str] = Field(None, description="Why relevant or not relevant")
    prestige_score: Optional[int] = Field(None, ge=1, le=10, description="1-10 prestige of opportunity/venue (for invitations/reviews)")
    prestige_reason: Optional[str] = Field(None, description="Why prestigious or not")
    
    # Event/Deadline Info (for invitations/reviews)
    event_date: Optional[str] = Field(None, description="Event date YYYY-MM-DD (for invitations)")
    deadline: Optional[str] = Field(None, description="Review/response deadline YYYY-MM-DD")
    location: Optional[str] = Field(None, description="Event location (for invitations)")
    time_commitment_hours: Optional[int] = Field(None, ge=0, description="Estimated hours required (0-1000)")
    time_commitment_reason: Optional[str] = Field(None, description="Why this time estimate")
    
    # Application-Specific Fields (from n8n and extended prompt)
    applicant_name: Optional[str] = Field(None, description="Applicant's name")
    applicant_email: Optional[str] = Field(None, description="Applicant's email address (from forwarded content if forwarded)")
    applicant_institution: Optional[str] = Field(None, description="Current institution")
    scientific_excellence_score: Optional[int] = Field(None, ge=1, le=10, description="1-10 academic credentials strength (university, publications, skills)")
    scientific_excellence_reason: Optional[str] = Field(None, description="Why strong or weak credentials")
    research_fit_score: Optional[int] = Field(None, ge=1, le=10, description="1-10 research alignment with lab areas")
    research_fit_reason: Optional[str] = Field(None, description="Why good or poor research fit")
    recommendation_score: Optional[int] = Field(None, ge=1, le=10, description="1-10 overall recommendation strength (for applications)")
    recommendation_reason: Optional[str] = Field(None, description="Why this overall recommendation")
    
    # Extended Application Fields (from updated prompt)
    nationality: Optional[str] = Field(None, description="Applicant nationality if mentioned")
    highest_degree_completed: Optional[str] = Field(None, description="B.Sc./M.Sc./PhD/etc.")
    current_situation: Optional[str] = Field(None, description="Student/employment situation")
    recent_thesis_title: Optional[str] = Field(None, description="Recent thesis/project title")
    recommendation_source: Optional[str] = Field(None, description="Name of person who recommended")
    
    # Academic Trajectory Fields (for normalized scoring)
    expected_graduation_year: Optional[int] = Field(None, description="Expected graduation year (or PhD completion year for postdocs)")
    is_fast_track: Optional[bool] = Field(None, description="PhD only: true if B.Sc.+M.Sc. â‰¤5 years total")
    program_intensity_note: Optional[str] = Field(None, description="Note about program intensity (e.g., 'ETH M.Sc.', 'Extended for research', '3-year M.Tech.')")
    has_industry_research_experience: Optional[bool] = Field(None, description="Has research experience in industry/biotech (may explain lack of publications)")
    years_since_phd: Optional[float] = Field(None, description="Postdocs only: years since PhD completion")
    
    # Online Profiles
    github_account: Optional[str] = Field(None, description="GitHub username or URL")
    linkedin_account: Optional[str] = Field(None, description="LinkedIn username or URL")
    google_scholar_account: Optional[str] = Field(None, description="Google Scholar username or URL")
    
    # Technical Experience (scores 0-10 with evidence)
    coding_experience: Optional[TechnicalExperience] = Field(None, description="Coding experience with score and evidence")
    omics_genomics_experience: Optional[TechnicalExperience] = Field(None, description="Omics/genomics experience with score and evidence")
    medical_data_experience: Optional[TechnicalExperience] = Field(None, description="Medical data experience with score and evidence")
    sequence_analysis_algorithms_experience: Optional[TechnicalExperience] = Field(None, description="Sequence analysis experience with score and evidence")
    image_analysis_experience: Optional[TechnicalExperience] = Field(None, description="Image analysis experience with score and evidence")
    
    # Profile Tags (with confidence and reason)
    profile_tags: Optional[List[ProfileTag]] = Field(None, description="List of profile tags with confidence and reason")
    
    # Red Flags
    is_mass_email: Optional[bool] = Field(None, description="Generic template email")
    no_research_background: Optional[bool] = Field(None, description="No research experience mentioned")
    irrelevant_field: Optional[bool] = Field(None, description="Field unrelated to lab")
    possible_spam: Optional[bool] = Field(None, description="Spam/phishing indicators")
    insufficient_materials: Optional[bool] = Field(None, description="Application lacks sufficient documentation (e.g., CV only, no cover letter/research statement)")
    prompt_manipulation_detected: Optional[bool] = Field(None, description="Attempts to manipulate evaluation via prompt injection")
    prompt_manipulation_indicators: Optional[List[str]] = Field(None, description="Specific manipulation patterns detected")
    is_not_application: Optional[bool] = Field(None, description="Email is NOT actually an application but was misclassified (e.g., recommendation, general question about process)")
    correct_category: Optional[str] = Field(None, description="If is_not_application=true, the correct category this email should be classified as (e.g., 'work-colleague', 'work-other')")
    is_not_application_reason: Optional[str] = Field(None, description="If is_not_application=true, explain why this email is not an application (e.g., 'Email from colleague introducing their student', 'General question about application deadlines', 'Request for information about the lab')")
    
    # Evaluation Details
    key_strengths: Optional[List[str]] = Field(None, description="Key strengths if strong candidate")
    concerns: Optional[List[str]] = Field(None, description="Specific concerns or issues")
    next_steps: Optional[str] = Field(None, description="Recommended next action")
    additional_notes: Optional[str] = Field(None, description="Other relevant observations")
    
    # Additional Information Request (for promising candidates with insufficient data)
    should_request_additional_info: Optional[bool] = Field(None, description="Should request additional information? Only true if candidate has potential for 8-10 recommendation score but lacks sufficient information")
    missing_information_items: Optional[List[str]] = Field(None, description="List of specific information that is missing (e.g., 'Full CV', 'Research proposal', 'Transcripts', 'Letters of recommendation')")
    potential_recommendation_score: Optional[int] = Field(None, ge=8, le=10, description="Potential recommendation score (8-10) if missing information were provided. Only set if should_request_additional_info is true")
    
    # Information Sources
    information_used: Optional[InformationUsed] = Field(None, description="Which documents were used for evaluation")
    
    # Receipt-Specific Fields
    vendor: Optional[str] = Field(None, description="Vendor name (for receipts)")
    amount: Optional[str] = Field(None, description="Amount paid (for receipts)")
    currency: Optional[str] = Field(None, description="Currency CHF/USD/EUR (for receipts)")
    
    # Followup Detection (from n8n)
    is_followup: bool = Field(default=False, description="Is this a followup to previous email?")
    followup_to_date: Optional[str] = Field(None, description="YYYY-MM-DD of original email")
    
    # Cold Email Detection
    is_cold_email: bool = Field(default=False, description="Unsolicited cold email?")
    
    # Suggestions
    suggested_folder: Optional[str] = Field(None, description="Recommended folder")
    suggested_labels: Optional[List[str]] = Field(default=None, description="Recommended labels")
    
    # Sender Identity
    curated_sender_name: Optional[str] = Field(
        None, 
        description="The extracted real-world name of the sender (e.g., 'Amazon AWS', 'John Smith', 'Apple Invoice'). Use the email body/signature if header is generic."
    )
    
    # Draft Responses (from n8n - array of response options with tone)
    # Using Optional for OpenAI structured outputs compatibility
    answer_options: Optional[List[Dict[str, str]]] = Field(
        default=None,
        description="Array of draft responses: [{text: str, tone: str}]. Tone can be: positive, decline, inquiry, neutral"
    )


class AIClassifier:
    """
    AI-powered email classifier using LangChain.
    Prompts adapted from n8n workflow for consistency.
    
    v3.0: Supports category-aware model selection via config file.
    """
    
    def __init__(self, provider: Optional[str] = None, model: Optional[str] = None, temperature: float = 0.1, db_session=None, source: str = "cli", category_hint: Optional[str] = None, config_path: Optional[str] = None):
        """
        Initialize AI classifier.
        
        Args:
            provider: "openai", "azure", or "anthropic" (default: from LLM_PROVIDER env var, fallback to "openai")
            model: Model name (default: gpt-4o-mini for OpenAI, claude-3-haiku for Anthropic)
            temperature: 0-1 (lower = more deterministic)
            db_session: Optional database session for cost tracking
            source: Source of API calls ('cli' or 'api')
            category_hint: Email category for model selection (NEW in v3.0)
            config_path: Path to model_routing.yaml (NEW in v3.0)
        """
        # NEW v3.0: Load configuration and category hint
        self.category_hint = category_hint
        self.config_loader = None
        
        # Load config if category hint provided or if config file exists
        if category_hint or config_path or self._config_exists():
            try:
                from backend.core.ai.config_loader import ConfigLoader
                self.config_loader = ConfigLoader(config_path)
            except Exception as e:
                logger.debug(f"Could not load config: {e}")
        
        # NEW v3.0: Category-based model selection
        if category_hint and self.config_loader and not (provider and model):
            config = self.config_loader.get_config_for_category(category_hint)
            # Don't default provider here - let get_model_config() auto-detect from llm_endpoints.yaml
            provider = config.get("provider")  # Can be None, will be resolved later
            model = config.get("model", "gpt-5-mini")
            temperature = config.get("temperature", temperature)
        
        # Determine model name first
        if model:
            self.model_name = model
        elif provider == "anthropic" or os.getenv("LLM_PROVIDER", "").lower() == "anthropic":
            self.model_name = os.getenv("ANTHROPIC_MODEL", "claude-3-haiku-20240307")
        else:
            self.model_name = os.getenv("OPENAI_MODEL", "gpt-5-mini")
        
        # Get provider config from llm_endpoints.yaml (or fall back to env vars)
        from backend.core.ai.llm_config import get_model_config
        cfg_provider, cfg_api_key, cfg_endpoint, cfg_api_version = get_model_config(self.model_name)
        
        # Use explicit provider if given, otherwise use config (default to azure since all models use Azure)
        self.provider = provider or cfg_provider or os.getenv("LLM_PROVIDER", "azure").lower()
        
        # GPT-5 models only support temperature=1.0
        if self.model_name and self.model_name.startswith("gpt-5"):
            if temperature != 1.0:
                temperature = 1.0
        
        self.temperature = temperature
        
        # Cost tracking (in-memory)
        self.total_prompt_tokens = 0
        self.total_completion_tokens = 0
        self.total_cost = 0.0
        self.total_cached_tokens = 0  # Track cached tokens for savings reporting
        
        # Database cost tracking (optional)
        self.db_cost_tracker = None
        if db_session:
            try:
                from backend.core.database.cost_tracking import CostTracker
                self.db_cost_tracker = CostTracker(db_session, source=source)
            except Exception as e:
                logger.warning(f"Could not initialize database cost tracker: {e}")
        
        # Initialize LLM using config credentials
        if self.provider == "openai":
            api_key = cfg_api_key or os.getenv("OPENAI_API_KEY")
            if not api_key:
                raise ValueError("OPENAI_API_KEY not found in environment")
            
            self.openai_client = OpenAI(api_key=api_key, base_url=cfg_endpoint)
            self.llm = ChatOpenAI(model=self.model_name, temperature=temperature, api_key=api_key, base_url=cfg_endpoint)
            self.use_structured_outputs = True
            
        elif self.provider == "azure":
            from openai import AzureOpenAI
            
            api_key = cfg_api_key or os.getenv("AZURE_OPENAI_API_KEY")
            if not api_key:
                raise ValueError(f"Azure API key not found for model '{self.model_name}'")
            
            azure_endpoint = cfg_endpoint or os.getenv("AZURE_OPENAI_ENDPOINT")
            if not azure_endpoint:
                raise ValueError(f"Azure endpoint not found for model '{self.model_name}'")
            
            api_version = cfg_api_version or os.getenv("AZURE_OPENAI_API_VERSION", "2025-04-01-preview")
            
            self.openai_client = AzureOpenAI(
                api_key=api_key,
                api_version=api_version,
                azure_endpoint=azure_endpoint
            )
            
            from langchain_openai import AzureChatOpenAI
            self.llm = AzureChatOpenAI(
                azure_deployment=self.model_name,
                api_version=api_version,
                azure_endpoint=azure_endpoint,
                api_key=api_key,
                temperature=temperature
            )
            self.use_structured_outputs = True
            
        elif self.provider == "anthropic":
            api_key = cfg_api_key or os.getenv("ANTHROPIC_API_KEY")
            if not api_key:
                raise ValueError("ANTHROPIC_API_KEY not found in environment")
            self.llm = ChatAnthropic(model=self.model_name, temperature=temperature, api_key=api_key)
            self.openai_client = None
            self.use_structured_outputs = False
        else:
            raise ValueError(f"Unknown provider: {self.provider}. Use 'openai', 'azure', or 'anthropic'")
        
        # Output parser (fallback for Anthropic or if structured outputs fail)
        self.parser = JsonOutputParser(pydantic_object=AIClassificationResult)
    
    def _config_exists(self) -> bool:
        """Check if model routing config file exists (NEW in v3.0)."""
        from pathlib import Path
        config_path = Path(__file__).parent / "config" / "model_routing.yaml"
        local_path = config_path.with_suffix('.local.yaml')
        return config_path.exists() or local_path.exists()
    
    def set_cost_tracker_session(self, db_session, source: str = "cli"):
        """
        Set database session for cost tracking (can be called after initialization).
        
        Args:
            db_session: Database session
            source: Source of API calls ('cli' or 'api')
        """
        if db_session and not self.db_cost_tracker:
            try:
                from backend.core.database.cost_tracking import CostTracker
                self.db_cost_tracker = CostTracker(db_session, source=source)
            except Exception as e:
                logger.debug(f"Could not set cost tracker: {e}")
    
    def _calculate_cost(self, prompt_tokens: int, completion_tokens: int, cached_tokens: int = 0) -> float:
        """
        Calculate API cost based on model pricing, accounting for cached tokens.
        
        OpenAI Pricing (as of Nov 2025):
        - gpt-4o: $5.00/1M input, $15.00/1M output, $2.50/1M cached (50% off)
        - gpt-4o-mini: $0.150/1M input, $0.600/1M output, $0.075/1M cached (50% off)
        - gpt-4.1 series: $2.00/1M input, $10.00/1M output, $0.50/1M cached (75% off)
        - gpt-5 series: Similar discounts
        
        Anthropic Pricing (no caching yet):
        - claude-3-opus: $15.00/1M input, $75.00/1M output
        - claude-3-sonnet: $3.00/1M input, $15.00/1M output
        - claude-3-haiku: $0.25/1M input, $1.25/1M output
        
        Args:
            prompt_tokens: Total prompt tokens (including cached)
            completion_tokens: Output tokens
            cached_tokens: Tokens served from cache (discounted)
        """
        # Pricing table (per 1M tokens)
        pricing = {
            'gpt-4o': {'input': 5.00, 'output': 15.00, 'cached': 2.50},  # 50% off cached
            'gpt-4o-mini': {'input': 0.150, 'output': 0.600, 'cached': 0.075},  # 50% off
            'gpt-4o-2024-11-20': {'input': 2.50, 'output': 10.00, 'cached': 1.25},  # 50% off
            'gpt-4.1': {'input': 2.00, 'output': 10.00, 'cached': 0.50},  # 75% off
            'gpt-4.1-mini': {'input': 0.40, 'output': 1.60, 'cached': 0.10},  # 75% off
            'gpt-4.1-nano': {'input': 0.10, 'output': 0.40, 'cached': 0.025},  # 75% off
            'gpt-5': {'input': 10.00, 'output': 30.00, 'cached': 5.00},  # 50% off (estimated)
            'gpt-5-mini': {'input': 0.25, 'output': 1.00, 'cached': 0.125},  # 50% off (estimated)
            'gpt-5-nano': {'input': 0.10, 'output': 0.40, 'cached': 0.050},  # 50% off (estimated)
            'gpt-5.1': {'input': 15.00, 'output': 60.00, 'cached': 7.50},  # 50% off (estimated)
            'gpt-3.5-turbo': {'input': 0.50, 'output': 1.50, 'cached': 0.25},  # 50% off
            # Anthropic (no caching support yet)
            'claude-3-opus-20240229': {'input': 15.00, 'output': 75.00, 'cached': 15.00},
            'claude-3-sonnet-20240229': {'input': 3.00, 'output': 15.00, 'cached': 3.00},
            'claude-3-haiku-20240307': {'input': 0.25, 'output': 1.25, 'cached': 0.25},
        }
        
        # Get pricing for current model (default to gpt-4o-mini if unknown)
        model_pricing = pricing.get(self.model_name, pricing['gpt-4o-mini'])
        
        # Calculate cost with cache discount
        uncached_tokens = prompt_tokens - cached_tokens
        uncached_cost = (uncached_tokens / 1_000_000) * model_pricing['input']
        cached_cost = (cached_tokens / 1_000_000) * model_pricing.get('cached', model_pricing['input'])
        output_cost = (completion_tokens / 1_000_000) * model_pricing['output']
        
        total_cost = uncached_cost + cached_cost + output_cost
        
        # Update tracking
        self.total_prompt_tokens += prompt_tokens
        self.total_completion_tokens += completion_tokens
        self.total_cost += total_cost
        
        return total_cost
    
    def get_usage_stats(self) -> Dict[str, Any]:
        """Get cumulative usage and cost statistics."""
        stats = {
            'provider': self.provider,
            'model': self.model_name,
            'total_prompt_tokens': self.total_prompt_tokens,
            'total_completion_tokens': self.total_completion_tokens,
            'total_tokens': self.total_prompt_tokens + self.total_completion_tokens,
            'total_cost_usd': round(self.total_cost, 4),
            'avg_tokens_per_email': round((self.total_prompt_tokens + self.total_completion_tokens) / max(1, self.total_prompt_tokens // 1000), 0) if self.total_prompt_tokens > 0 else 0
        }
        
        # Add cache statistics if available
        if self.total_cached_tokens > 0:
            cache_hit_rate = (self.total_cached_tokens / self.total_prompt_tokens) * 100 if self.total_prompt_tokens > 0 else 0
            stats['total_cached_tokens'] = self.total_cached_tokens
            stats['cache_hit_rate_percent'] = round(cache_hit_rate, 1)
            # Note: Cost already reflects caching discount from OpenAI API
            stats['cache_note'] = 'Cost already discounted by OpenAI (50-90% off cached tokens)'
        
        return stats
    
    async def classify_ab_test(
        self,
        email: ProcessedEmail,
        models: Optional[List[str]] = None,
        sender_history: Optional[Dict] = None,
        timeout_seconds: int = 30
    ) -> Dict[str, Any]:
        """
        Run A/B test with multiple models and return results.
        
        NEW in v3.0: Built-in A/B testing for model comparison.
        
        Args:
            email: Email to classify
            models: List of model names to test (default from constants)
            sender_history: Optional sender history
            timeout_seconds: Timeout per model classification
            
        Returns:
            Dict mapping model name to classification result or error
            
        Example:
            results = await classifier.classify_ab_test(
                email, 
                models=["gpt-4o-mini", "gpt-4o", "claude-3-5-sonnet-20241022"]
            )
            # Returns: {
            #     "gpt-4o-mini": <AIClassificationResult>,
            #     "gpt-4o": <AIClassificationResult>,
            #     "claude-3-5-sonnet-20241022": {"error": "..."}  # if failed
            # }
        """
        if models is None:
            from .config_constants import AB_TEST_DEFAULT_MODELS
            models = AB_TEST_DEFAULT_MODELS
        
        logger.info(f"Starting A/B test for email {email.uid} with models: {models}")
        results = {}
        
        # Run classification with each model in parallel
        import asyncio
        
        async def classify_with_model(model: str):
            """Helper to classify with a specific model with timeout."""
            try:
                classifier = AIClassifier(
                    model=model,
                    category_hint=self.category_hint
                )
                result = await asyncio.wait_for(
                    classifier.classify(email, sender_history),
                    timeout=timeout_seconds
                )
                return model, result, None
            except asyncio.TimeoutError:
                logger.error(f"A/B test timeout for {model} after {timeout_seconds}s")
                return model, None, f"Timeout after {timeout_seconds}s"
            except Exception as e:
                logger.error(f"A/B test failed for {model}: {e}")
                return model, None, str(e)
        
        # Run all classifications in parallel
        tasks = [classify_with_model(model) for model in models]
        model_results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Collect results
        for item in model_results:
            if isinstance(item, Exception):
                logger.error(f"A/B test task failed: {item}")
                continue
            
            model, result, error = item
            if error:
                results[model] = {"error": error}
            else:
                results[model] = result
        
        # Log comparison
        self._log_ab_comparison(email.uid, results)
        
        logger.info(
            f"A/B test complete for email {email.uid}: "
            f"{len([r for r in results.values() if not isinstance(r, dict) or 'error' not in r])} successful, "
            f"{len([r for r in results.values() if isinstance(r, dict) and 'error' in r])} failed"
        )
        
        return results
    
    def _log_ab_comparison(self, email_uid: str, results: Dict[str, Any]):
        """
        Log A/B test results to file for analysis.
        
        Results are appended to ab_test_results.jsonl in JSONL format.
        Uses file locking to prevent corruption from concurrent writes.
        """
        import json
        from pathlib import Path
        from datetime import datetime
        import fcntl  # For file locking
        
        from .config_constants import AB_TEST_RESULTS_FILE
        
        log_entry = {
            "timestamp": datetime.utcnow().isoformat(),
            "email_uid": email_uid,
            "category_hint": self.category_hint,
            "results": {}
        }
        
        for model, result in results.items():
            if isinstance(result, dict) and "error" in result:
                log_entry["results"][model] = result
            else:
                # Extract key fields from AIClassificationResult
                log_entry["results"][model] = {
                    "category": result.category,
                    "confidence": result.confidence,
                    "summary": result.summary[:100] if result.summary else None,
                    "urgency": result.urgency,
                    "urgency_score": result.urgency_score
                }
        
        # Append to JSONL file with file locking
        log_path = Path(AB_TEST_RESULTS_FILE)
        try:
            with open(log_path, "a") as f:
                # Acquire exclusive lock for writing
                fcntl.flock(f.fileno(), fcntl.LOCK_EX)
                try:
                    f.write(json.dumps(log_entry) + "\n")
                    f.flush()  # Ensure written to disk
                finally:
                    # Release lock
                    fcntl.flock(f.fileno(), fcntl.LOCK_UN)
            
            logger.debug(f"A/B test logged to {log_path}")
        except Exception as e:
            logger.error(f"Failed to log A/B test results: {e}")
        
        logger.info(f"âœ… AI Classifier initialized: {self.provider}/{self.model_name}")
    
    def _build_prompt(self, email: ProcessedEmail, sender_history: Optional[Dict] = None, sender_email_history: Optional[List[Dict]] = None, reference_letters: Optional[List[Dict]] = None) -> str:
        """
        Build classification prompt from email data.
        Mirrors n8n workflow prompt structure.
        
        v3.0: Checks prompt registry for category-specific prompts.
        
        Maps ProcessedEmail fields to n8n prompt variables:
        - {{ date }} â†’ email.date
        - {{ subject }} â†’ email.subject
        - {{ from }} â†’ email.from_address + email.from_name
        - {{ to }} â†’ email.to_addresses
        - {{ content }} â†’ email.body_markdown
        - {{ attachment_count }} â†’ email.attachment_count
        - {{ attachment_summaries }} â†’ email.attachment_texts
        
        Args:
            email: ProcessedEmail object
            sender_history: Optional sender statistics dict
            sender_email_history: Optional list of recent emails from same sender
        """
        # NEW v3.0: Check if we have a registered category-specific prompt
        if self.category_hint:
            try:
                from backend.core.ai.prompts import get_prompt_for_category
                prompt_func = get_prompt_for_category(self.category_hint)
                if prompt_func:
                    logger.debug(f"Using registered prompt for category: {self.category_hint}")
                    # Pass sender_email_history and reference_letters if prompt function accepts them
                    import inspect
                    sig = inspect.signature(prompt_func)
                    kwargs = {}
                    if 'sender_email_history' in sig.parameters:
                        kwargs['sender_email_history'] = sender_email_history
                    if 'reference_letters' in sig.parameters:
                        kwargs['reference_letters'] = reference_letters
                    if kwargs:
                        return prompt_func(email, sender_history, **kwargs)
                    else:
                        return prompt_func(email, sender_history)
            except Exception as e:
                logger.debug(f"Could not load category prompt: {e}, using default")
        
        # DEFAULT PROMPT (used when no category-specific prompt is registered)
        # Format sender (name + address like n8n)
        from_display = f"{email.from_name} <{email.from_address}>" if email.from_name else email.from_address
        
        # Format recipients
        to_display = ", ".join(email.to_addresses[:3])  # First 3 recipients
        if len(email.to_addresses) > 3:
            to_display += f" (+{len(email.to_addresses) - 3} more)"
        
        # Format attachments (like n8n shows attachment count and summaries)
        # Limit attachment summaries to prevent token explosion
        attachment_info = "No attachments"
        if email.has_attachments:
            # Note: For application emails, attachment count may include consolidated attachments from prior emails
            attachment_info = f"{email.attachment_count} attachment(s) in current email"
            if email.attachment_texts:
                # Limit attachments (lower than application-specific processing)
                summaries = []
                for i, text in enumerate(email.attachment_texts[:MAX_ATTACHMENTS_FOR_CLASSIFICATION], 1):
                    # Truncate each attachment summary
                    if len(text) > MAX_ATTACHMENT_SUMMARY_CHARS:
                        summary = text[:MAX_ATTACHMENT_SUMMARY_CHARS] + f"... [truncated {len(text) - MAX_ATTACHMENT_SUMMARY_CHARS:,} chars]"
                        logger.debug(f"Truncated attachment {i} from {len(text):,} to {MAX_ATTACHMENT_SUMMARY_CHARS:,} chars")
                    else:
                        summary = text
                    summaries.append(f"Attachment {i}: {summary}")
                attachment_info += "\n\n" + "\n\n".join(summaries)
                
                # Warn if we skipped attachments
                if len(email.attachment_texts) > MAX_ATTACHMENTS_FOR_CLASSIFICATION:
                    attachment_info += f"\n\n[{len(email.attachment_texts) - MAX_ATTACHMENTS_FOR_CLASSIFICATION} more attachments not shown]"
        
        # Sender context (if available)
        sender_context = ""
        if sender_history:
            sender_context = f"""
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SENDER STATISTICS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â€¢ Previous emails: {sender_history.get('email_count', 0)}
â€¢ Typical category: {sender_history.get('typical_category', 'unknown')}
â€¢ Sender type: {sender_history.get('sender_type', 'unknown')}
â€¢ Is frequent sender: {sender_history.get('is_frequent', False)}
â€¢ Your avg response time: {sender_history.get('avg_reply_time_hours', 'unknown')} hours
â€¢ Last email: {sender_history.get('last_seen', 'never')}
"""
        
        # Add recent emails from same sender for context (if available)
        sender_email_context = ""
        if sender_email_history:
            emails_with_content = sender_email_history.get('emails_with_content', [])
            emails_without_content = sender_email_history.get('emails_without_content', [])
            total_count = sender_email_history.get('total_count', 0)
            
            if emails_with_content or emails_without_content or total_count > 0:
                sender_email_context = "\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
                sender_email_context += "SENDER EMAIL HISTORY (Previous emails from same sender):\n"
                sender_email_context += "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n"
                sender_email_context += "Use this history to detect patterns like follow-ups, repeated messages, or context from prior conversations.\n\n"
                
                # Show up to 10 most recent emails with full content summaries
                if emails_with_content:
                    sender_email_context += "â–¶ Most Recent Emails (with content summaries):\n"
                    for i, hist_email in enumerate(emails_with_content[:10], 1):
                        sender_email_context += f"\n  Email #{i}:\n"
                        sender_email_context += f"    From: {hist_email.get('from', 'Unknown')}\n"
                        sender_email_context += f"    To: {hist_email.get('to', 'Unknown')}\n"
                        sender_email_context += f"    Date: {hist_email.get('date', 'Unknown date')}\n"
                        sender_email_context += f"    Subject: {hist_email.get('subject', 'No subject')}\n"
                        sender_email_context += f"    Summary: {hist_email.get('content_summary', 'No summary available')}\n"
                
                # Show additional emails without content (just date and subject)
                # Limit to show at most 10 total emails (with_content + without_content)
                shown_with_content = min(len(emails_with_content), 10)
                remaining_slots = 10 - shown_with_content
                if emails_without_content and remaining_slots > 0:
                    sender_email_context += "\nâ–¶ Additional Recent Emails (date and subject only):\n"
                    for i, hist_email in enumerate(emails_without_content[:remaining_slots], 1):
                        sender_email_context += f"    â€¢ {hist_email.get('date', 'Unknown date')}: {hist_email.get('subject', 'No subject')}\n"
                
                # Show total count
                shown_total = shown_with_content + min(len(emails_without_content), remaining_slots)
                if total_count > shown_total:
                    additional_count = total_count - shown_total
                    sender_email_context += f"\nâ–¶ Total emails from this sender: {total_count} (showing {shown_total} most recent, {additional_count} older emails not shown)\n"
                elif total_count > 0:
                    sender_email_context += f"\nâ–¶ Total emails from this sender: {total_count}\n"
                
                sender_email_context += "\n"
        
        # Build full email content with intelligent truncation
        raw_content = email.body_markdown or email.body_text or "(empty)"
        email_content, was_truncated = _truncate_email_content(raw_content)
        
        # Log if we had to truncate
        if was_truncated:
            original_len = len(raw_content)
            truncated_len = len(email_content)
            estimated_tokens = original_len // CHARS_PER_TOKEN_CONSERVATIVE
            logger.warning(
                f"Email content truncated: {original_len:,} â†’ {truncated_len:,} chars "
                f"(~{estimated_tokens:,} tokens would have exceeded limit)"
            )
        
        # Load prompt from YAML config (supports CONFIG_DIR overlay)
        # System prompt contains the classification instructions
        # User template contains the email details
        date_str = email.date.strftime('%Y-%m-%d %H:%M:%S')

        # Get system prompt with variable substitution
        system_prompt = get_prompt("classifier.system_prompt", default="")

        # Get user template and format with email details
        user_template = get_prompt(
            "classifier.user_template",
            default="",
            date_str=date_str,
            from_display=from_display,
            to_display=to_display,
            subject=email.subject,
            attachment_info=attachment_info,
            sender_context=sender_context,
            sender_email_context=sender_email_context,
            email_content=email_content
        )

        # Combine system prompt and user template
        prompt = system_prompt + "\n\n" + user_template
        
        # Note: When using structured outputs, we don't include the schema in the prompt
        # The schema is sent separately via the API, saving ~1900 tokens!
        # 
        # PROMPT CACHING OPTIMIZATION:
        # All instructions (first ~2000 tokens) are identical across emails
        # Email details (last ~500 tokens) vary per email
        # OpenAI automatically caches the common prefix
        # Cache discount: 50% (GPT-4o/5) or 75% (GPT-4.1) on cached tokens
        
        return prompt
    
    async def classify(self, 
                      email: ProcessedEmail,
                      sender_history: Optional[Dict] = None,
                      sender_email_history: Optional[List[Dict]] = None,
                      reference_letters: Optional[List[Dict]] = None,
                      max_retries: int = 2) -> AIClassificationResult:
        """
        Classify email using AI with retry logic and fallback.
        
        Args:
            email: Processed email to classify
            sender_history: Optional sender history from database
            sender_email_history: Optional list of recent emails from same sender
            max_retries: Number of retries on transient failures
            
        Returns:
            AIClassificationResult with structured classification
        """
        last_error = None
        
        for attempt in range(max_retries + 1):
            try:
                # Build prompt (WITHOUT JSON schema if using structured outputs)
                prompt_text = self._build_prompt(email, sender_history, sender_email_history, reference_letters)

                # DEBUG: Log prompt to file for comparison
                import os
                prompt_log_dir = os.getenv("PROMPT_LOG_DIR", "")
                if prompt_log_dir:
                    from pathlib import Path
                    log_dir = Path(prompt_log_dir)
                    log_dir.mkdir(parents=True, exist_ok=True)
                    prompt_file = log_dir / f"prompt_{email.uid}.txt"
                    with open(prompt_file, "w") as f:
                        f.write(f"=== SYSTEM MESSAGE ===\n")
                        f.write("You are an expert email classifier for an academic researcher in biomedical informatics.\n")
                        f.write(f"\n=== USER MESSAGE ===\n")
                        f.write(prompt_text)
                    logger.debug(f"Logged prompt to {prompt_file}")

                start_time = time.time()
                
                # Use OpenAI Structured Outputs (native API) if available
                if self.use_structured_outputs and self.openai_client:
                    # Run OpenAI API call in a thread to avoid blocking the async event loop
                    # This allows true parallel processing when using --parallel-workers
                    import asyncio
                    def _call_openai():
                        return self.openai_client.beta.chat.completions.parse(
                            model=self.model_name,
                            messages=[
                                {"role": "system", "content": "You are an expert email classifier for an academic researcher in biomedical informatics."},
                                {"role": "user", "content": prompt_text}
                            ],
                            response_format=AIClassificationResult,  # Pydantic model!
                            temperature=self.temperature
                        )
                    response = await asyncio.to_thread(_call_openai)
                    
                    elapsed = time.time() - start_time
                    
                    # Validate response format before accessing .choices
                    if isinstance(response, str):
                        # Azure sometimes returns raw string on transient errors
                        raise ValueError(f"Unexpected string response from API (expected ChatCompletion object): {response[:200]}...")
                    if not hasattr(response, 'choices') or not response.choices:
                        raise ValueError(f"Invalid response format - no choices: {type(response).__name__}")
                    
                    # Extract parsed result (OpenAI does the parsing!)
                    classification = response.choices[0].message.parsed
                    
                    # Extract usage data
                    if response.usage:
                        prompt_tokens = response.usage.prompt_tokens
                        completion_tokens = response.usage.completion_tokens
                        total_tokens = response.usage.total_tokens
                        
                        # NEW: Check for prompt caching stats (OpenAI API v1.0+)
                        cached_tokens = 0
                        if hasattr(response.usage, 'prompt_tokens_details'):
                            cached_tokens = getattr(response.usage.prompt_tokens_details, 'cached_tokens', 0)
                        
                        # Calculate cost (with cache discount if applicable)
                        cost = self._calculate_cost(prompt_tokens, completion_tokens, cached_tokens)
                        
                        # Track cached tokens
                        if cached_tokens > 0:
                            self.total_cached_tokens += cached_tokens
                        
                        # Log with cache stats (at debug level to reduce noise - shown in card output instead)
                        if cached_tokens > 0:
                            cache_hit_rate = (cached_tokens / prompt_tokens) * 100
                            logger.debug(
                                f"ðŸ’° CACHE HIT! {cached_tokens}/{prompt_tokens} tokens cached ({cache_hit_rate:.0f}%) | "
                                f"Total: {total_tokens} tokens, cost: ${cost:.4f}"
                            )
                        else:
                            logger.debug(
                                f"AI API usage: {total_tokens} tokens "
                                f"(prompt: {prompt_tokens}, completion: {completion_tokens}), "
                                f"cost: ${cost:.4f}"
                            )
                        
                        # Log to database if tracker available
                        if self.db_cost_tracker:
                            try:
                                self.db_cost_tracker.log_usage(
                                    model=self.model_name,
                                    task='classification',
                                    prompt_tokens=prompt_tokens,
                                    completion_tokens=completion_tokens,
                                    total_tokens=total_tokens,
                                    cost_usd=cost,
                                    email_id=None,  # Will be set by caller if needed
                                    context_data={
                                        'category': classification.category,
                                        'confidence': classification.confidence,
                                        'urgency': classification.urgency
                                    }
                                )
                            except Exception as e:
                                logger.debug(f"Failed to log usage to database: {e}")
                    
                    # Log at debug level - shown in card output instead
                    logger.debug(f"AI classified email {email.uid}: {classification.category} "
                              f"(confidence: {classification.confidence:.2f}, {elapsed:.1f}s)")
                    
                    return classification
                    
                else:
                    # Fallback to LangChain (for Anthropic or if structured outputs unavailable)
                    prompt = ChatPromptTemplate.from_messages([
                        ("system", "You are an expert email classifier for an academic researcher in biomedical informatics."),
                        ("human", "{input}\n\nOUTPUT (strict JSON format):\n{format_instructions}")
                    ])
                    
                    chain = prompt | self.llm | self.parser
                    
                    response = await chain.ainvoke({
                        "input": prompt_text,
                        "format_instructions": self.parser.get_format_instructions()
                    })
                    elapsed = time.time() - start_time
                    
                    # LangChain already parsed it
                    classification = AIClassificationResult(**response)
                    
                    logger.info(f"AI classified email {email.uid}: {classification.category} "
                              f"(confidence: {classification.confidence:.2f}, {elapsed:.1f}s)")
                    
                    return classification
                
            except ValidationError as e:
                # Pydantic validation error - output format mismatch or score out of range
                logger.warning(f"AI output validation failed for {email.uid} (attempt {attempt + 1}): {e}")
                last_error = e
                
                if attempt < max_retries:
                    # Retry with explicit validation message
                    logger.info(f"Retrying with stricter JSON validation requirements...")
                    delay = 2 ** attempt
                    time.sleep(delay)
                else:
                    logger.error(f"Validation failed after {max_retries + 1} attempts")
                    break
                
            except OutputParserException as e:
                # LangChain failed to parse JSON
                logger.warning(f"AI output parsing failed for {email.uid} (attempt {attempt + 1}): {e}")
                last_error = e
                
                if attempt < max_retries:
                    # Retry - LLM might return valid JSON on second try
                    logger.info(f"Retrying JSON parsing...")
                    delay = 2 ** attempt
                    time.sleep(delay)
                else:
                    logger.error(f"JSON parsing failed after {max_retries + 1} attempts")
                    break
                
            except Exception as e:
                # Other errors (rate limit, network, etc.) - retry
                last_error = e
                error_type = type(e).__name__
                logger.warning(f"AI classification attempt {attempt + 1}/{max_retries + 1} failed for {email.uid}: {error_type}: {e}")
                
                if attempt < max_retries:
                    # Exponential backoff for rate limits
                    delay = 2 ** attempt
                    logger.info(f"Retrying in {delay}s...")
                    time.sleep(delay)
                else:
                    logger.error(f"AI classification failed after {max_retries + 1} attempts for {email.uid}")
        
        # Fallback to simple classification
        logger.warning(f"Using fallback classification for {email.uid}")
        return AIClassificationResult(
            category="work-other",  # Conservative fallback
            confidence=0.0,
            reasoning=f"AI classification failed after {max_retries + 1} attempts: {str(last_error)}",
            urgency="normal",
            urgency_score=5,
            urgency_reason="Default due to classification error",
            summary=email.subject or "No subject",
            action_items=[],
            needs_reply=False,
            sentiment="neutral",
            is_cold_email=False,
            is_followup=False
        )
    
    async def classify_batch(self, 
                            emails: List[ProcessedEmail],
                            sender_histories: Optional[Dict[str, Dict]] = None) -> List[AIClassificationResult]:
        """
        Classify multiple emails in batch (for efficiency).
        
        Args:
            emails: List of emails to classify
            sender_histories: Dict mapping email address to sender history
            
        Returns:
            List of classification results (same order as input)
        """
        results = []
        
        for email in emails:
            sender_hist = sender_histories.get(email.from_address) if sender_histories else None
            result = await self.classify(email, sender_hist)
            results.append(result)
        
        return results

